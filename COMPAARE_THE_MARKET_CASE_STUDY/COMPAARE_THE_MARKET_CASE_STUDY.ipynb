{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "jo3qnjswrhgztag6vayx",
   "authorId": "4560704223481",
   "authorName": "NAVEYBIANCA",
   "authorEmail": "naveybianca@gmail.com",
   "sessionId": "84a404f8-6465-4aa9-8ace-05131cd9f007",
   "lastEditTime": 1751327601708
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "python_imports"
   },
   "source": "# Import python packages\nimport streamlit as st\nimport pandas as pd\nimport numpy as np\n\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n\ndf = session.table(\"ctm.public.nba\")\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "812403be-0e2e-4232-9547-7cb76f5cfecb",
   "metadata": {
    "language": "sql",
    "name": "describe_table"
   },
   "outputs": [],
   "source": "describe table nba;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ccff0d7a-83dd-45a2-9326-c7e0e73df56c",
   "metadata": {
    "name": "handling_Null",
    "collapsed": false
   },
   "source": "# Summary Null Handling Strategy by Column\n\n- **CustomerID (Identifier)**\n  - **Strategy:** Drop row or disregard\n  - **Rationale:** CustomerID serves as a unique identifier and is critical for data integrity and accurate record linkage. Missing values could compromise join operations and result in unreliable analyses.\n\n- **Age (Continuous)**\n  - **Strategy:** Impute with median or mean\n  - **Rationale:** Age is a continuous variable. Imputation with the median is preferred to mitigate the influence of outliers, while the mean provides a suitable alternative for normally distributed data, ensuring robust statistical analysis.\n\n- **Segment (Categorical)**\n  - **Strategy:** Impute with mode or introduce a “Missing” category\n  - **Rationale:** Segment is a categorical variable. Imputing with the mode preserves the most representative value, while adding a “Missing” category maintains transparency regarding unknown values for further investigation.\n\n- **Recency (Categorical)**\n     - **Strategy:** Impute with mode or introduce a \"Missing\" category\n    - **Rationale:** Recency is a categorical segmentation variable with defined engagement states (Active,     Inactive, Dormant). Imputing with the mode preserves the most common engagement pattern while maintaining data distribution integrity. Alternatively, introducing a \"Missing\" category retains data completeness and allows for potential analysis of unclassified customer behavior patterns.\n \n- **PriorInterest (Binary)**\n  - **Strategy:** Impute with 0 (no interest)\n  - **Rationale:** PriorInterest is a binary variable. Imputing missing values as 0 (no interest) is a conservative approach that avoids overestimating customer engagement and maintains model stability.\n\n- **DidPurchase (Binary)**\n  - **Strategy:** Exclude rows with missing values (if used as the modeling target)\n  - **Rationale:** DidPurchase is the target variable for predictive modeling. Rows with missing values must be omitted to ensure the integrity of model training and avoid bias from incomplete outcomes.\n"
  },
  {
   "cell_type": "code",
   "id": "27ef6142-5034-447f-be66-28f585a7931e",
   "metadata": {
    "language": "sql",
    "name": "segment_nulls"
   },
   "outputs": [],
   "source": "--Are there null values in segment column\nselect segment, count(segment)\nFROM ctm.public.nba\nGROUP BY segment;-- 493 null values found",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1222e7e4-d6e2-4844-8a20-3edfa3affa46",
   "metadata": {
    "language": "sql",
    "name": "validate_segment_nulls"
   },
   "outputs": [],
   "source": "--Are there null values in segment column\nselect count(segment)\nFROM ctm.public.nba\nwhere segment = 'NULL';-- 493 null values found",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b0f4c2b4-3692-42c1-b4ec-2285c1eb7b8f",
   "metadata": {
    "language": "sql",
    "name": "update_segment_nulls"
   },
   "outputs": [],
   "source": "UPDATE ctm.public.nba\nSET segment = 'Missing'\nWHERE segment = 'NULL'; --updated to new category 'Missing'",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b13d22b5-2671-4430-9217-9964b74e00bb",
   "metadata": {
    "language": "sql",
    "name": "validate_age"
   },
   "outputs": [],
   "source": "select count(age)\nfrom nba\nwhere age is null; -- no missing values",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "923cf12c-87a2-45fb-bafc-6d6e6eebf509",
   "metadata": {
    "language": "sql",
    "name": "validate_customerid_nulls"
   },
   "outputs": [],
   "source": "select sum(customerid)\nfrom nba\nwhere customerid is null; -- no missing values",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b93ab977-ae9a-488a-bff6-b2b3c5b116c7",
   "metadata": {
    "language": "sql",
    "name": "recency_nulls"
   },
   "outputs": [],
   "source": "select recency, count(recency)\nFROM ctm.public.nba\nGROUP BY recency;-- 493 null values found & deleted",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "86beb394-a0e6-4929-81db-cda09fee16bc",
   "metadata": {
    "language": "sql",
    "name": "update_recency_nulls"
   },
   "outputs": [],
   "source": "UPDATE ctm.public.nba\nSET recency = 'Missing'\nWHERE recency = 'NULL'; --updated to new category 'Missing'",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "db87e816-5abf-4132-bf29-c308f4d72bf3",
   "metadata": {
    "language": "sql",
    "name": "validate_didpurchase_nulls"
   },
   "outputs": [],
   "source": "select didpurchase, count(didpurchase)\nFROM ctm.public.nba\nGROUP BY didpurchase-- only 0 and 1 found / no null values found ",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8db759e2-0a05-4558-8c00-58bff0c4d609",
   "metadata": {
    "language": "sql",
    "name": "validate_priorInterest_nulls"
   },
   "outputs": [],
   "source": "select priorinterest, count(priorinterest)\nFROM ctm.public.nba\nGROUP BY priorinterest-- no null values found ",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2ed28c2c-b9e8-4c62-80f0-b5e59a9695f9",
   "metadata": {
    "name": "Exploratory_Analysis",
    "collapsed": false
   },
   "source": "## Key metrics:\n\n- Overall purchase rate (DidPurchase mean)\n- Purchase rate by Segment, Recency, and Age cohorts\n\n\nStatistical relationships:\n\n\n- Correlation between PriorInterest and DidPurchase\n- Conversion lift for high-interest vs. low-interest customers\n\n\nVisualizations:\n\n\n- Bar charts: Purchase rate by segment/recency , age bins\n- Heatmap: Correlation matrix of features\n"
  },
  {
   "cell_type": "code",
   "id": "cbb9868a-bfb4-4c3d-aee2-93e444a01da7",
   "metadata": {
    "language": "sql",
    "name": "mean_puchase_conversion"
   },
   "outputs": [],
   "source": "-- Purchase conversion rates by mean\nSELECT \n  ROUND(AVG(DidPurchase) * 100, 2) || '%' AS conversion_rate\nFROM ctm.public.nba;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "039e2d2d-20fa-43a0-ad1d-f5b9f7fd5a66",
   "metadata": {
    "language": "sql",
    "name": "purchase_conversion_rate_by_segment"
   },
   "outputs": [],
   "source": "-- Purchase conversion rates by segment\nSELECT \n  Segment, \n  ROUND(AVG(DidPurchase) * 100, 2) || '%' AS conversion_rate\nFROM ctm.public.nba\nGROUP BY Segment\nORDER BY conversion_rate DESC;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ab1e4388-19e6-450b-9a30-80f3e51098bb",
   "metadata": {
    "language": "python",
    "name": "viz_purchase_cr_by_segment"
   },
   "outputs": [],
   "source": "import matplotlib.pyplot as plt\n\n# Data\nsegments = ['A', 'B', 'C']\nconversion_rates = [51.41, 32.17, 21.81]\n\n# Create bar chart\nplt.figure(figsize=(8, 5))\nbars = plt.bar(segments, conversion_rates, color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n\n# Add data labels\nfor bar in bars:\n    height = bar.get_height()\n    plt.text(bar.get_x() + bar.get_width()/2, height,\n             f'{height:.2f}%', ha='center', va='bottom')\n\nplt.title('Purchase Conversion Rates by Segment')\nplt.xlabel('Segment')\nplt.ylabel('Conversion Rate (%)')\nplt.ylim(0, 60)\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()\n\n# What the Chart Shows\n\n# Segment A has the highest conversion rate (51.41%)\n# Segment B is intermediate (32.17%)\n# Segment C has the lowest (21.81%)\n\n# Insight\n# Targeting Segment A offers the highest potential for conversions.\n# Segments B and C may benefit from tailored engagement strategies to boost their conversion rates.",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "aa7d2f1f-d0a1-4fc4-8637-d452cf1f0b7a",
   "metadata": {
    "language": "sql",
    "name": "purchase_cr_by_recency"
   },
   "outputs": [],
   "source": "-- Purchase conversion rates by recency\nSELECT \n  recency, \n  ROUND(AVG(DidPurchase) * 100, 2) || '%' AS conversion_rate\nFROM ctm.public.nba\nGROUP BY recency\nORDER BY conversion_rate DESC;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ae47a067-b2f7-400b-9047-b3c6bd8f142f",
   "metadata": {
    "language": "python",
    "name": "viz_purchase_cr_by_recency"
   },
   "outputs": [],
   "source": "import pandas as pd\n\nrecency_data = pd.DataFrame({\n    'recency': ['Active', 'Inactive', 'Dormant'],\n    'conversion_rate': [41.38, 20.07, 12.04]\n})\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(8, 5))\nbars = plt.bar(recency_data['recency'], recency_data['conversion_rate'], color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n\n# Add data labels\nfor bar in bars:\n    height = bar.get_height()\n    plt.text(bar.get_x() + bar.get_width()/2, height,\n             f'{height:.2f}%', ha='center', va='bottom')\n\nplt.title('Purchase Conversion Rates by Recency Segment')\nplt.xlabel('Recency Segment')\nplt.ylabel('Conversion Rate (%)')\nplt.ylim(0, 60)\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()\n\n# Targeting Active customers is most effective, while Inactive and Dormant segments may require reactivation strategies to boost conversions.",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a7e84255-ac5c-4896-b418-2c56b1cf0c8a",
   "metadata": {
    "language": "sql",
    "name": "age_binned_cr"
   },
   "outputs": [],
   "source": "-- Purchase conversion rates by segment\nWITH binned_data AS (\n  SELECT \n    CASE\n      WHEN age <= 20 THEN '0-20'\n      WHEN age <= 30 THEN '21-30'\n      WHEN age <= 40 THEN '31-40'\n      WHEN age <= 50 THEN '41-50'\n      WHEN age <= 60 THEN '51-60'\n      WHEN age <= 70 THEN '61-70'\n      ELSE '71-80'\n    END AS age_bin,\n    AVG(DidPurchase) * 100 AS conversion_rate\n  FROM ctm.public.nba\n  GROUP BY age_bin\n)\nSELECT \n  age_bin,\n  ROUND(conversion_rate, 2) || '%' AS conversion_rate\nFROM binned_data\nORDER BY conversion_rate DESC;\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c5498869-34f3-464b-b9a5-47421e3458d2",
   "metadata": {
    "language": "python",
    "name": "viz_age_bin_cr"
   },
   "outputs": [],
   "source": "import matplotlib.pyplot as plt\n\n# Data from your query\nage_bins = ['0-20', '21-30', '31-40', '41-50', '51-60', '61-70', '71-80']\nconversion_rates = [12.38, 30.34, 36.47, 32.91, 24.54, 15.73, 11.71]\n\n# Create chart\nplt.figure(figsize=(10, 6))\nbars = plt.bar(age_bins, conversion_rates, color='skyblue')\n\n# Add data labels\nfor bar in bars:\n    height = bar.get_height()\n    plt.text(bar.get_x() + bar.get_width()/2, height,\n             f'{height:.2f}%', ha='center', va='bottom')\n\n# Formatting\nplt.title('Conversion Rate by Age Group', fontsize=14)\nplt.xlabel('Age Group', fontsize=12)\nplt.ylabel('Conversion Rate (%)', fontsize=12)\nplt.ylim(0, 40)\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()\n\n# Key Insights from Binned Data\n# Peak Conversion:\n\n# Ages 31-40 have highest conversion (36.47%)\n\n# Followed by 21-30 (30.34%) and 41-50 (32.91%)\n\n# Lowest Conversion:\n\n# Seniors (71-80) convert at 11.71%\n\n# Young adults (0-20) at 12.38%\n\n# Business Implications:\n\n# Target marketing to 31-50 age groups\n\n# Investigate low conversion in youth/senior segments\n\n# Create age-specific product recommendations",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a8e46871-928c-40fd-916b-e7f7f1fde26d",
   "metadata": {
    "language": "sql",
    "name": "corr_priorInterest_didPurchase"
   },
   "outputs": [],
   "source": "SELECT \n  CORR(PRIORINTEREST, DIDPURCHASE) AS correlation_coefficient\nFROM ctm.public.nba;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2a0ce182-d1f4-4370-81ba-cb54f2c1885d",
   "metadata": {
    "language": "python",
    "name": "corr_py"
   },
   "outputs": [],
   "source": "import pandas as pd\nfrom snowflake.snowpark.context import get_active_session\n\nsession = get_active_session()\ndf = session.table(\"ctm.public.nba\").to_pandas()\n\ncorrelation = df['PRIORINTEREST'].corr(df['DIDPURCHASE'])\nprint(f\"Correlation: {correlation:.4f}\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "31e55b30-40c4-4464-a8be-8589c3179f73",
   "metadata": {
    "language": "python",
    "name": "comparing_corr_by_segment"
   },
   "outputs": [],
   "source": "import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Fetch data for segments A, B, C\nquery = \"\"\"\nSELECT \n    SEGMENT,\n    CORR(DIDPURCHASE, PRIORINTEREST) AS correlation\nFROM ctm.public.nba\nWHERE SEGMENT IN ('A', 'B', 'C')\n    AND PRIORINTEREST IS NOT NULL\n    AND DIDPURCHASE IS NOT NULL\nGROUP BY SEGMENT\n\"\"\"\n\ncorr_df = session.sql(query).to_pandas()\nprint(corr_df)\nplt.figure(figsize=(10, 6))\nax = sns.barplot(\n    x='SEGMENT', \n    y='CORRELATION', \n    data=corr_df,\n    palette='viridis'\n)\n\n# Add data labels\nfor p in ax.patches:\n    ax.annotate(f\"{p.get_height():.3f}\", \n                (p.get_x() + p.get_width() / 2., p.get_height()),\n                ha='center', va='center', \n                xytext=(0, 10), \n                textcoords='offset points')\n\nplt.title('Correlation: DIDPURCHASE vs. PRIORINTEREST by Segment', fontsize=14)\nplt.xlabel('Segment', fontsize=12)\nplt.ylabel('Correlation Coefficient', fontsize=12)\nplt.ylim(0, 1)\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9f8a67af-b7cb-45a8-80df-5357247c39e2",
   "metadata": {
    "language": "python",
    "name": "corr_by_recency"
   },
   "outputs": [],
   "source": "import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom snowflake.snowpark.context import get_active_session\n\n# Execute query\ncorr_df = session.sql(\"\"\"\n    SELECT \n        RECENCY,\n        CORR(DIDPURCHASE, PRIORINTEREST) AS correlation\n    FROM ctm.public.nba\n    WHERE RECENCY IN ('Active', 'Inactive', 'Dormant')\n        AND PRIORINTEREST IS NOT NULL\n        AND DIDPURCHASE IS NOT NULL\n    GROUP BY RECENCY\n    ORDER BY correlation DESC\n\"\"\").to_pandas()\n\n# Plot results\nplt.figure(figsize=(10, 6))\nax = sns.barplot(\n    x='RECENCY', \n    y='CORRELATION', \n    data=corr_df,\n    palette='coolwarm'\n)\n\n# Add data labels\nfor p in ax.patches:\n    ax.annotate(f\"{p.get_height():.3f}\", \n                (p.get_x() + p.get_width() / 2., p.get_height()),\n                ha='center', va='center', \n                xytext=(0, 10), \n                textcoords='offset points',\n                fontsize=10)\n\nplt.title('Correlation: DIDPURCHASE vs. PRIORINTEREST by Recency', fontsize=14)\nplt.xlabel('Recency Category', fontsize=12)\nplt.ylabel('Correlation Coefficient', fontsize=12)\nplt.ylim(0, 1)\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "54478229-9879-4200-9bb9-f1140be3d883",
   "metadata": {
    "language": "sql",
    "name": "sql_conversion_lift"
   },
   "outputs": [],
   "source": "WITH rates AS (\n  SELECT\n    PriorInterest,\n    AVG(DidPurchase) AS conversion_rate\n  FROM ctm.public.nba\n  GROUP BY PriorInterest\n)\nSELECT \n  (MAX(CASE WHEN PriorInterest = 1 THEN conversion_rate END) \n   - MAX(CASE WHEN PriorInterest = 0 THEN conversion_rate END))\n  / MAX(CASE WHEN PriorInterest = 0 THEN conversion_rate END) \n  * 100 AS conversion_lift_percent\nFROM rates;\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4b8640cd-41fa-486b-b4b4-a3806c3a0651",
   "metadata": {
    "language": "python",
    "name": "py_conversion_lift"
   },
   "outputs": [],
   "source": "high_interest_rate = df[df['PRIORINTEREST'] == 1]['DIDPURCHASE'].mean()\nlow_interest_rate = df[df['PRIORINTEREST'] == 0]['DIDPURCHASE'].mean()\n\nconversion_lift = ((high_interest_rate - low_interest_rate) / low_interest_rate) * 100\nprint(f\"Conversion Lift: {conversion_lift:.2f}%\")\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "97a2eb74-d7f9-4e22-890f-be9b3c453c15",
   "metadata": {
    "language": "python",
    "name": "p_value_pi_dp"
   },
   "outputs": [],
   "source": "from scipy.stats import ttest_ind\n\ngroup1 = df[df['PRIORINTEREST'] == 1]['DIDPURCHASE']\ngroup0 = df[df['PRIORINTEREST'] == 0]['DIDPURCHASE']\n\nt_stat, p_value = ttest_ind(group1, group0)\nprint(f\"p-value: {p_value:.4f}\")  # p < 0.05 indicates significant difference\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "439c9fe9-ffa7-4e90-bf71-a329e5d4415a",
   "metadata": {
    "name": "Quadrant_summary",
    "collapsed": false
   },
   "source": "## **Quadrant Names for PriorInterest vs. DidPurchase**\n\n| PriorInterest | DidPurchase | Quadrant Name (Descriptive) | Quadrant Name (Creative)        |\n|---------------|-------------|-----------------------------|---------------------------------|\n| 0             | 0           | Uninterested & Didn't Buy   | Missed Opportunity              |\n| 0             | 1           | Uninterested & Bought       | Surprise Win                    |\n| 1             | 0           | Interested & Didn't Buy     | Lost Conversion                 |\n| 1             | 1           | Interested & Bought         | Conversion Success              |\n\n## **Variations & Suggestions**\n\n- **Uninterested & Didn't Buy (0,0):**\n  - **Missed Opportunity**\n  - **Low Intent, No Action**\n  - **Passive Audience**\n- **Uninterested & Bought (0,1):**\n  - **Surprise Win**\n  - **Impulse Purchase**\n  - **Unexpected Conversion**\n- **Interested & Didn't Buy (1,0):**\n  - **Lost Conversion**\n  - **High Intent, No Sale**\n  - **Abandoned Cart**\n- **Interested & Bought (1,1):**\n  - **Conversion Success**\n  - **High Intent, High Action**\n  - **Ideal Customer**\n\n## **Example Table with Quadrant Names**\n\n| Quadrant Name     | PriorInterest | DidPurchase | Description                      |\n|-------------------|---------------|-------------|----------------------------------|\n| Missed Opportunity| 0             | 0           | No interest, no purchase         |\n| Surprise Win      | 0             | 1           | No interest, but purchased       |\n| Lost Conversion   | 1             | 0           | Interested, but didn't purchase  |\n| Conversion Success| 1             | 1           | Interested and purchased         |\n"
  },
  {
   "cell_type": "markdown",
   "id": "5e028431-2069-45dc-9212-8976557dbb52",
   "metadata": {
    "name": "customer_groups_recommendations",
    "collapsed": false
   },
   "source": "## **Summary Table**\n\n| Quadrant Name     | Targeting Strategy Examples                                  |\n|-------------------|-------------------------------------------------------------|\n| Lost Conversion   | Retargeting ads, abandoned cart emails, dynamic creatives   |\n| Conversion Success| Upsell/cross-sell, loyalty programs, personalized follow-up |\n| Missed Opportunity| Awareness campaigns, educational content, lookalike targeting|\n| Surprise Win      | Post-purchase engagement, feedback requests, cross-sell     |"
  },
  {
   "cell_type": "code",
   "id": "fc2f21c8-e296-456d-8212-4facbc5570f3",
   "metadata": {
    "language": "sql",
    "name": "pi_dp_count"
   },
   "outputs": [],
   "source": "SELECT \n    PRIORINTEREST, \n    DIDPURCHASE, \n    COUNT(DISTINCT CUSTOMERID) AS customer_count\nFROM ctm.public.nba\nGROUP BY PRIORINTEREST, DIDPURCHASE\nORDER BY PRIORINTEREST, DIDPURCHASE",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8264e3fd-4a8a-423f-bcfc-9f23edeb9a46",
   "metadata": {
    "language": "sql",
    "name": "pi_dp_percentage"
   },
   "outputs": [],
   "source": "WITH counts AS (\n  SELECT \n    PRIORINTEREST, \n    DIDPURCHASE, \n    COUNT(DISTINCT CUSTOMERID) AS customer_count\n  FROM ctm.public.nba\n  GROUP BY PRIORINTEREST, DIDPURCHASE\n),\ntotal AS (\n  SELECT SUM(customer_count) AS total_customers\n  FROM counts\n)\nSELECT \n  counts.PRIORINTEREST, \n  counts.DIDPURCHASE, \n  counts.customer_count,\n  round((counts.customer_count / total.total_customers) * 100 ,2) || '%' AS percentage\nFROM counts, total\nORDER BY PRIORINTEREST, DIDPURCHASE;\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "93814a89-ced7-46d7-8a1b-00301d585d49",
   "metadata": {
    "language": "sql",
    "name": "lost_conversion_by_all_varibles"
   },
   "outputs": [],
   "source": "-- 2602 (2.6%) customers showed interest but did not purchase\n\nWITH interest_no_purchase AS (\n  SELECT \n    CUSTOMERID,\n    SEGMENT,\n    RECENCY,\n    CASE\n      WHEN AGE < 20 THEN '0-19'\n      WHEN AGE BETWEEN 20 AND 29 THEN '20-29'\n      WHEN AGE BETWEEN 30 AND 39 THEN '30-39'\n      WHEN AGE BETWEEN 40 AND 49 THEN '40-49'\n      WHEN AGE BETWEEN 50 AND 59 THEN '50-59'\n      ELSE '60+'\n    END AS age_bin\n  FROM ctm.public.nba\n  WHERE PRIORINTEREST = 1 AND DIDPURCHASE = 0\n)\nSELECT \n  SEGMENT,\n  age_bin,\n  RECENCY,\n  COUNT(DISTINCT CUSTOMERID) AS customer_count,\n  ROUND((COUNT(DISTINCT CUSTOMERID) / 2602.0) * 100, 1) AS percent_of_group\nFROM interest_no_purchase\nGROUP BY SEGMENT, age_bin, RECENCY\nORDER BY customer_count DESC;\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0d37c5d8-617d-419a-aa84-75b52c3d38eb",
   "metadata": {
    "language": "sql",
    "name": "lost_conversion_by_segment"
   },
   "outputs": [],
   "source": "SELECT\n  SEGMENT,\n  COUNT(DISTINCT CUSTOMERID) AS customer_count,\n  ROUND((COUNT(DISTINCT CUSTOMERID) / 2602.0) * 100, 1) || '%' AS percent_of_group\nFROM ctm.public.nba\nWHERE PRIORINTEREST = 1 AND DIDPURCHASE = 0\nGROUP BY SEGMENT\nORDER BY customer_count DESC;\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "940ef075-c4e7-4f01-a5f0-3c0a888e21ab",
   "metadata": {
    "language": "sql",
    "name": "lost_conversion_by_recency"
   },
   "outputs": [],
   "source": "SELECT\n  RECENCY,\n  COUNT(DISTINCT CUSTOMERID) AS customer_count,\n  ROUND((COUNT(DISTINCT CUSTOMERID) / 2602.0) * 100, 1)  || '%' AS percent_of_group\nFROM ctm.public.nba\nWHERE PRIORINTEREST = 1 AND DIDPURCHASE = 0\nGROUP BY RECENCY\nORDER BY customer_count DESC;\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1b91321d-9e3f-4460-a166-e537f283b89e",
   "metadata": {
    "language": "sql",
    "name": "lost_conversion_by_age"
   },
   "outputs": [],
   "source": "SELECT\n  CASE\n    WHEN AGE < 20 THEN '0-19'\n    WHEN AGE BETWEEN 20 AND 29 THEN '20-29'\n    WHEN AGE BETWEEN 30 AND 39 THEN '30-39'\n    WHEN AGE BETWEEN 40 AND 49 THEN '40-49'\n    WHEN AGE BETWEEN 50 AND 59 THEN '50-59'\n    ELSE '60+'\n  END AS age_bin,\n  COUNT(DISTINCT CUSTOMERID) AS customer_count,\n  ROUND((COUNT(DISTINCT CUSTOMERID) / 2602.0) * 100, 1) || '%' AS percent_of_group\nFROM ctm.public.nba\nWHERE PRIORINTEREST = 1 AND DIDPURCHASE = 0\nGROUP BY age_bin\nORDER BY customer_count DESC;\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4c8d0ee4-a645-4117-82dd-0e6047a558bb",
   "metadata": {
    "language": "python",
    "name": "ct_pi_dp"
   },
   "outputs": [],
   "source": "# contingency table data \nct = pd.DataFrame({\n    '0': [67398, 2602],  # Did Not Purchase\n    '1': [23424, 6576]    # Did Purchase\n}, index=['0', '1'])  # Index: Prior Interest\n\n# Rename rows and columns for clarity\nct.index = ['No Prior Interest', 'Prior Interest']\nct.columns = ['Did Not Purchase', 'Did Purchase']\n# Plot heatmap\nplt.figure(figsize=(8, 6))\nsns.heatmap(ct, annot=True, fmt='d', cmap='Blues')\nplt.xlabel('Purchase Status')\nplt.ylabel('Prior Interest')\nplt.title('Prior Interest vs. Purchase Counts')\nplt.show()\n\nprint(ct)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d2da0498-4c83-45b3-bf07-45d8bcc5faff",
   "metadata": {
    "language": "python",
    "name": "vis_lift_pi_dp"
   },
   "outputs": [],
   "source": "import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\n# Ensure column names are ALL CAPS\ndf.columns = df.columns.str.upper()\n\n# Calculate conversion rates\nconversion_rates = df.groupby('PRIORINTEREST')['DIDPURCHASE'].mean().reset_index()\nconversion_rates['PRIORINTEREST'] = conversion_rates['PRIORINTEREST'].map(\n    {0: 'No Prior Interest', 1: 'Prior Interest'}\n)\n\n# Plot\nplt.figure(figsize=(8, 5))\nax = sns.barplot(\n    x='PRIORINTEREST', \n    y='DIDPURCHASE', \n    hue='PRIORINTEREST',  # Assign x variable to hue\n    data=conversion_rates, \n    palette='viridis',\n    legend=False  # Disable legend to avoid duplication\n)\n\n# Calculate and annotate lift\nhigh_rate = conversion_rates.loc[conversion_rates['PRIORINTEREST'] == 'Prior Interest', 'DIDPURCHASE'].values[0]\nlow_rate = conversion_rates.loc[conversion_rates['PRIORINTEREST'] == 'No Prior Interest', 'DIDPURCHASE'].values[0]\nlift = (high_rate - low_rate) / low_rate * 100\n\n# Position annotation correctly\nx_pos = conversion_rates[conversion_rates['PRIORINTEREST'] == 'Prior Interest'].index[0]\nplt.text(x_pos, high_rate + 0.02, f\"Lift: +{lift:.1f}%\", \n         ha='center', fontsize=12, fontweight='bold')\n\n# Formatting\nplt.title('Conversion Rate by Prior Interest', fontsize=14)\nplt.xlabel('Prior Interest Level', fontsize=12)\nplt.ylabel('Conversion Rate', fontsize=12)\nplt.ylim(0, min(1, high_rate * 1.3))  # Dynamic upper limit\nplt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f'{y:.0%}'))\nplt.tight_layout()\nplt.show()\n\n#both customers have been on the website but customer that visit with priot interest are 177% more likely to purchase than first time users",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "abebdd20-7c87-4f9f-98cf-b7d9badb4d05",
   "metadata": {
    "name": "Predictive_Modeling",
    "collapsed": false
   },
   "source": "## Predictive Modeling\nTrain purchase propensity model:\n- Features: Age, Segment, Recency, PriorInterest\n- Target: DidPurchase\n\nAlgorithm: Logistic Regression or Random Forest\n\nEvaluate model:\n\n- Metrics: AUC-ROC, precision-recall\n\n\nFeature importance analysis (e.g., SHAP values)\n"
  },
  {
   "cell_type": "code",
   "id": "a86c2abd-49cb-476f-ae4e-ce5da892ba99",
   "metadata": {
    "language": "python",
    "name": "sklean_prepare"
   },
   "outputs": [],
   "source": "from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\n\n\n# Load Snowflake table into Snowpark DataFrame\ndf_snowpark = session.table(\"ctm.public.nba\")\n\n# Convert to Pandas DataFrame\ndf = df_snowpark.to_pandas()\n\n# Prepare categorical features\ncategorical_cols = ['SEGMENT', 'RECENCY']\n\n# Initialize OneHotEncoder with correct parameters\nencoder = OneHotEncoder(drop='first', sparse_output=False)  # Use sparse_output instead of sparse\n\n# Fit and transform categorical features\ncategorical_features = encoder.fit_transform(df[categorical_cols])\n\n# Create DataFrame for encoded features\ncategorical_df = pd.DataFrame(categorical_features, \n                              columns=encoder.get_feature_names_out(categorical_cols))\n\n# Combine features\nX = pd.concat([\n    df[['AGE', 'PRIORINTEREST']],\n    categorical_df\n], axis=1)\ny = df['DIDPURCHASE']\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\nprint(\"Training set size:\", X_train.shape[0])\nprint(\"Test set size:\", X_test.shape[0])\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1ce09877-0432-4b45-871e-16a644d9e1cd",
   "metadata": {
    "language": "python",
    "name": "train_model"
   },
   "outputs": [],
   "source": "from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_auc_score, average_precision_score\nimport matplotlib.pyplot as plt\nfrom sklearn.calibration import calibration_curve\n\n# Initialize and train model\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel.fit(X_train, y_train)\n\n# Generate predicted probabilities and class predictions\ny_pred_proba = model.predict_proba(X_test)[:, 1]  # Probability of positive class\ny_pred = model.predict(X_test)  # Class predictions at default threshold (0.5)\n\n# Calculate metrics\nroc_auc = roc_auc_score(y_test, y_pred_proba)\npr_auc = average_precision_score(y_test, y_pred_proba)\n\nprint(f\"ROC-AUC: {roc_auc:.4f}\")\nprint(f\"PR-AUC: {pr_auc:.4f}\")\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "432e5928-a776-4d42-82c7-919998453c67",
   "metadata": {
    "language": "python",
    "name": "confusion_matrix"
   },
   "outputs": [],
   "source": "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\n\n# Generate confusion matrix\ncm = confusion_matrix(y_test, y_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, \n                              display_labels=['No Purchase', 'Purchase'])\ndisp.plot(cmap='Blues', values_format='d')\nplt.title('Confusion Matrix (Threshold = 0.5)')\nplt.show()\n\n# Top-left: True Negatives (correct non-churn predictions)\n# Bottom-right: True Positives (correct churn predictions)\n# Top-right: False Positives (Type I errors)\n# Bottom-left: False Negatives (Type II errors)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ade1f012-ff9e-4898-a0f8-4bfc2375c35f",
   "metadata": {
    "name": "evaluating_rf_model",
    "collapsed": false
   },
   "source": "Evaluating the model"
  },
  {
   "cell_type": "code",
   "id": "ca3e6b55-76b4-4409-ae24-52a4ca069981",
   "metadata": {
    "language": "python",
    "name": "roc_curve"
   },
   "outputs": [],
   "source": "# 3. Threshold Analysis\n# Calculate ROC curve\nfrom sklearn.metrics import roc_curve, roc_auc_score\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\nroc_auc = roc_auc_score(y_test, y_pred_proba)\n# Plot ROC curve\nif callable(plt.plot):\n    plt.figure(figsize=(8, 6))\n    plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.3f})')\n    plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC Curve - Threshold Analysis')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\nelse:\n    print(\"Error: plt.plot is not callable. Restart kernel.\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c2f2a523-7477-4ad8-ba6e-4f741fd4a53f",
   "metadata": {
    "language": "python",
    "name": "calibration_plot"
   },
   "outputs": [],
   "source": "# Calibration plot\nprob_true, prob_pred = calibration_curve(y_test, y_pred_proba, n_bins=10)\nplt.figure(figsize=(8, 6))\nplt.plot(prob_pred, prob_true, marker='o', label='Random Forest')\nplt.plot([0, 1], [0, 1], linestyle='--', label='Perfectly calibrated')\nplt.xlabel('Predicted Probability')\nplt.ylabel('True Probability')\nplt.title('Calibration Curve')\nplt.legend()\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e2c57714-b31e-4c42-8097-7919485f416b",
   "metadata": {
    "language": "python",
    "name": "precision_recall"
   },
   "outputs": [],
   "source": "from sklearn.metrics import precision_recall_curve, average_precision_score\n\n# Calculate precision-recall curve\nprecision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba)\nap = average_precision_score(y_test, y_pred_proba)\n\n# Plot curve\nplt.figure(figsize=(8, 6))\nplt.plot(recall, precision, label=f'AP = {ap:.3f}')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve')\nplt.legend()\nplt.grid(True)\nplt.show()\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "16b5028d-7cc4-480b-b75f-daea7de9c21b",
   "metadata": {
    "language": "python",
    "name": "threshold_analysis"
   },
   "outputs": [],
   "source": "from sklearn.metrics import roc_curve, roc_auc_score\n\n# Print optimal threshold\noptimal_idx = np.argmax(tpr - fpr)\noptimal_threshold = thresholds[optimal_idx]\nprint(f\"Optimal Threshold: {optimal_threshold:.3f}\")\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8daaf631-2cb8-48ed-9942-5ac5a8ba5204",
   "metadata": {
    "language": "python",
    "name": "precision_tradeoff_threshold"
   },
   "outputs": [],
   "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n\n# Reload matplotlib to fix NoneType error\nimport importlib\nimport matplotlib\nimportlib.reload(matplotlib)\nimport matplotlib.pyplot as plt\n\n# Calculate metrics across thresholds\nthreshold_values = np.linspace(0, 1, 1000)\nmetrics = []\n\nfor thresh in threshold_values:\n    y_pred_thresh = (y_pred_proba >= thresh).astype(int)\n    tn, fp, fn, tp = confusion_matrix(y_test, y_pred_thresh).ravel()\n    \n    metrics.append({\n        'threshold': thresh,\n        'f1_score': f1_score(y_test, y_pred_thresh),\n        'precision': precision_score(y_test, y_pred_thresh, zero_division=0),\n        'recall': recall_score(y_test, y_pred_thresh),\n        'fpr': fp / (fp + tn) if (fp + tn) > 0 else 0\n    })\n\nmetrics_df = pd.DataFrame(metrics)\n\n# Create figure and axis explicitly\nfig, ax = plt.subplots(figsize=(10, 6))\nax.plot(metrics_df['threshold'], metrics_df['precision'], label='Precision', linewidth=2)\nax.plot(metrics_df['threshold'], metrics_df['recall'], label='Recall', linewidth=2)\nax.axvline(optimal_threshold, color='r', linestyle='--', label=f'Optimal Threshold ({optimal_threshold:.3f})')\nax.set_xlabel('Classification Threshold')\nax.set_ylabel('Score')\nax.set_title('Precision-Recall Tradeoff')\nax.legend()\nax.grid(True)\nplt.show()\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "06d9a7e2-ae88-4bf0-9908-53e42f3656a8",
   "metadata": {
    "language": "python",
    "name": "shap"
   },
   "outputs": [],
   "source": "import shap\n\n# Initialize SHAP explainer\nexplainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(X_test)\n\n# Check structure of shap_values (list of two arrays for binary classification)\nprint(f\"SHAP values structure: {type(shap_values)} with {len(shap_values)} elements\")\n\n# Global feature importance (use positive class index [1])\nshap.summary_plot(shap_values[1], X_test, plot_type=\"bar\")\n\n# Individual prediction explanation (first 3 samples)\nfor i in range(3):\n    shap.force_plot(\n        explainer.expected_value[1],\n        shap_values[1][i],  # Use positive class SHAP values\n        X_test.iloc[i],\n        matplotlib=True\n    )\n\nprint(f\"SHAP type: {type(shap_values)}\")\nprint(f\"SHAP length: {len(shap_values)}\")\nprint(f\"Element 0 shape: {shap_values[0].shape}\")\nprint(f\"Element 1 shape: {shap_values[1].shape}\")\n\nprint(f\"X_test features: {X_test.shape[1]}\")\nprint(f\"SHAP features: {shap_values[1].shape[1]}\")\n",
   "execution_count": null
  }
 ]
}